{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Data analysis and wrangling\nimport os\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as st\nfrom datetime import datetime\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns\n\n# Machine learning\nfrom sklearn import tree, metrics, svm\nfrom sklearn.metrics import accuracy_score, precision_score, confusion_matrix\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, learning_curve\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-12T17:36:38.643488Z","iopub.execute_input":"2022-07-12T17:36:38.644549Z","iopub.status.idle":"2022-07-12T17:36:38.651560Z","shell.execute_reply.started":"2022-07-12T17:36:38.644497Z","shell.execute_reply":"2022-07-12T17:36:38.650283Z"},"trusted":true},"execution_count":164,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-07-12T17:36:38.785895Z","iopub.execute_input":"2022-07-12T17:36:38.786708Z","iopub.status.idle":"2022-07-12T17:36:38.817181Z","shell.execute_reply.started":"2022-07-12T17:36:38.786667Z","shell.execute_reply":"2022-07-12T17:36:38.816313Z"},"trusted":true},"execution_count":165,"outputs":[]},{"cell_type":"code","source":"# Load the dataset\n\ntrain = pd.read_csv(\"/kaggle/input/titanic-machine-learning-from-disaster/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/titanic-machine-learning-from-disaster/test.csv\")\ntest_f = pd.read_csv(\"/kaggle/input/titanic-machine-learning-from-disaster/test.csv\")\n\npd.options.display.max_columns = 100\npd.options.display.max_rows = 100\n\ntrain","metadata":{"execution":{"iopub.status.busy":"2022-07-12T17:36:38.935688Z","iopub.execute_input":"2022-07-12T17:36:38.936168Z","iopub.status.idle":"2022-07-12T17:36:38.973836Z","shell.execute_reply.started":"2022-07-12T17:36:38.936137Z","shell.execute_reply":"2022-07-12T17:36:38.972914Z"},"trusted":true},"execution_count":166,"outputs":[]},{"cell_type":"code","source":"## Exploratory analysis ##\n\ntrain.info()\nprint('_'*40)\ntest.info()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T17:36:39.063394Z","iopub.execute_input":"2022-07-12T17:36:39.063849Z","iopub.status.idle":"2022-07-12T17:36:39.087806Z","shell.execute_reply.started":"2022-07-12T17:36:39.063813Z","shell.execute_reply":"2022-07-12T17:36:39.086260Z"},"trusted":true},"execution_count":167,"outputs":[]},{"cell_type":"code","source":"# What is the distribution of the \"numerical Features\" in the samples?\n\ntrain.describe().round(2)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T17:36:39.202387Z","iopub.execute_input":"2022-07-12T17:36:39.203033Z","iopub.status.idle":"2022-07-12T17:36:39.238995Z","shell.execute_reply.started":"2022-07-12T17:36:39.202979Z","shell.execute_reply":"2022-07-12T17:36:39.237798Z"},"trusted":true},"execution_count":168,"outputs":[]},{"cell_type":"code","source":"# What is the distribution of \"categorical features\" ?\n\ntrain.describe(include=['O'])","metadata":{"execution":{"iopub.status.busy":"2022-07-12T17:36:39.331196Z","iopub.execute_input":"2022-07-12T17:36:39.331641Z","iopub.status.idle":"2022-07-12T17:36:39.359058Z","shell.execute_reply.started":"2022-07-12T17:36:39.331606Z","shell.execute_reply":"2022-07-12T17:36:39.357425Z"},"trusted":true},"execution_count":169,"outputs":[]},{"cell_type":"code","source":"train.size","metadata":{"execution":{"iopub.status.busy":"2022-07-12T17:36:39.466506Z","iopub.execute_input":"2022-07-12T17:36:39.466922Z","iopub.status.idle":"2022-07-12T17:36:39.474700Z","shell.execute_reply.started":"2022-07-12T17:36:39.466889Z","shell.execute_reply":"2022-07-12T17:36:39.473832Z"},"trusted":true},"execution_count":170,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-12T17:36:39.586825Z","iopub.execute_input":"2022-07-12T17:36:39.587208Z","iopub.status.idle":"2022-07-12T17:36:39.594117Z","shell.execute_reply.started":"2022-07-12T17:36:39.587169Z","shell.execute_reply":"2022-07-12T17:36:39.593199Z"},"trusted":true},"execution_count":171,"outputs":[]},{"cell_type":"code","source":"train.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T17:36:39.726445Z","iopub.execute_input":"2022-07-12T17:36:39.727205Z","iopub.status.idle":"2022-07-12T17:36:39.736837Z","shell.execute_reply.started":"2022-07-12T17:36:39.727168Z","shell.execute_reply":"2022-07-12T17:36:39.735830Z"},"trusted":true},"execution_count":172,"outputs":[]},{"cell_type":"code","source":"# Correlation graph\n\nplt.style.use('fivethirtyeight')\n\nsns.set(font_scale=1.1)\ncorrelation_train = train.corr()\nmask = np.triu(correlation_train.corr())\nplt.figure(figsize=(10, 10))\nsns.heatmap(correlation_train,\n            annot=True,\n            fmt='.1f',\n            cmap='coolwarm',\n            square=True,\n            mask=mask,\n            linewidths=1,\n            cbar=False)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T17:36:39.856586Z","iopub.execute_input":"2022-07-12T17:36:39.856997Z","iopub.status.idle":"2022-07-12T17:36:40.127222Z","shell.execute_reply.started":"2022-07-12T17:36:39.856960Z","shell.execute_reply":"2022-07-12T17:36:40.125987Z"},"trusted":true},"execution_count":173,"outputs":[]},{"cell_type":"code","source":"\n\ngrid = sns.FacetGrid(train, row='Embarked', height=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T17:36:40.129823Z","iopub.execute_input":"2022-07-12T17:36:40.130245Z","iopub.status.idle":"2022-07-12T17:36:41.271559Z","shell.execute_reply.started":"2022-07-12T17:36:40.130212Z","shell.execute_reply":"2022-07-12T17:36:41.270141Z"},"trusted":true},"execution_count":174,"outputs":[]},{"cell_type":"code","source":"# Distribution of the price of the FARE and its relationship with the survival rate\n\nfigure = plt.figure(figsize=(18, 7))\nplt.hist([train[train['Survived'] == 1]['Fare'], train[train['Survived'] == 0]['Fare']], \n         stacked=True, color = ['g','r'],\n         bins = 50, label = ['Survived','Dead'])\nplt.xlabel('Fare')\nplt.ylabel('Number of passengers')\nplt.legend();","metadata":{"execution":{"iopub.status.busy":"2022-07-12T17:36:41.273270Z","iopub.execute_input":"2022-07-12T17:36:41.273581Z","iopub.status.idle":"2022-07-12T17:36:41.737486Z","shell.execute_reply.started":"2022-07-12T17:36:41.273551Z","shell.execute_reply":"2022-07-12T17:36:41.734827Z"},"trusted":true},"execution_count":175,"outputs":[]},{"cell_type":"code","source":"# Extract the target variable from the \"training dataset\"\ny = train['Survived'].reset_index(drop=True)\n\n# We join the data sets, so as not to repeat the operations\ndataset = pd.concat([train, test]).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T17:36:41.739396Z","iopub.execute_input":"2022-07-12T17:36:41.739930Z","iopub.status.idle":"2022-07-12T17:36:41.752963Z","shell.execute_reply.started":"2022-07-12T17:36:41.739880Z","shell.execute_reply":"2022-07-12T17:36:41.751518Z"},"trusted":true},"execution_count":176,"outputs":[]},{"cell_type":"code","source":"# We delete the features that are not useful (\"TICKET)\n\ndataset.drop(\"Ticket\", axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T17:36:41.755687Z","iopub.execute_input":"2022-07-12T17:36:41.756545Z","iopub.status.idle":"2022-07-12T17:36:41.763325Z","shell.execute_reply.started":"2022-07-12T17:36:41.756501Z","shell.execute_reply":"2022-07-12T17:36:41.762125Z"},"trusted":true},"execution_count":177,"outputs":[]},{"cell_type":"code","source":"## Treatment of NaN ##\n\n# We analyze the characteristic EMBARKED\n\nprint(dataset[dataset.Embarked == 'Q'].Pclass.value_counts())\nprint(dataset[dataset.Embarked == 'C'].Pclass.value_counts())\nprint(dataset[dataset.Embarked == 'S'].Pclass.value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-07-12T17:36:41.766073Z","iopub.execute_input":"2022-07-12T17:36:41.766753Z","iopub.status.idle":"2022-07-12T17:36:41.784922Z","shell.execute_reply.started":"2022-07-12T17:36:41.766687Z","shell.execute_reply":"2022-07-12T17:36:41.783529Z"},"trusted":true},"execution_count":178,"outputs":[]},{"cell_type":"code","source":"# We replace in 3 punctual null values\n\ndataset.at[61, 'Embarked'] = 'C'\ndataset.at[829, 'Embarked'] = 'C'\ndataset.at[1043, 'Fare'] = 13.5","metadata":{"execution":{"iopub.status.busy":"2022-07-12T17:36:41.786409Z","iopub.execute_input":"2022-07-12T17:36:41.788063Z","iopub.status.idle":"2022-07-12T17:36:41.794403Z","shell.execute_reply.started":"2022-07-12T17:36:41.788014Z","shell.execute_reply":"2022-07-12T17:36:41.793269Z"},"trusted":true},"execution_count":179,"outputs":[]},{"cell_type":"code","source":"# Fill in the AGE in the NaN\n\nprint(dataset.groupby(['Sex', 'Pclass'])['Age'].agg(['mean', 'median']).round(1))\n\nfor e in dataset[dataset.Age.isna()].index:\n    if (dataset.at[e,\"Sex\"] == \"female\") & dataset.at[e,\"Pclass\"] == \"1\":\n        dataset.at[e,\"Age\"] = 36.5\n    elif (dataset.at[e,\"Sex\"] == \"female\") & dataset.at[e,\"Pclass\"] == \"2\":\n        dataset.at[e,\"Age\"] = 27.6\n    elif (dataset.at[e,\"Sex\"] == \"female\") & dataset.at[e,\"Pclass\"] == \"3\":\n        dataset.at[e,\"Age\"] = 24.5\n    elif (dataset.at[e,\"Sex\"] == \"male\") & dataset.at[e,\"Pclass\"] == \"1\":\n        dataset.at[e,\"Age\"] = 39.3\n    elif (dataset.at[e,\"Sex\"] == \"male\") & dataset.at[e,\"Pclass\"] == \"2\":\n        dataset.at[e,\"Age\"] = 30.7\n    else:\n        dataset.at[e,\"Age\"] = 27.1","metadata":{"execution":{"iopub.status.busy":"2022-07-12T17:36:41.795426Z","iopub.execute_input":"2022-07-12T17:36:41.795979Z","iopub.status.idle":"2022-07-12T17:36:41.835023Z","shell.execute_reply.started":"2022-07-12T17:36:41.795939Z","shell.execute_reply":"2022-07-12T17:36:41.833765Z"},"trusted":true},"execution_count":180,"outputs":[]},{"cell_type":"code","source":"## create new features ##\n\n# FAMILY\n\ndataset['Family_size'] = dataset['SibSp'] + dataset['Parch'] + 1\ndataset['Alone'] = dataset['Family_size'].map(lambda s: 1 if s == 1 else 0)\ndataset['SmallFamily'] = dataset['Family_size'].map(lambda s: 1 if 2 <= s <= 3 else 0)\ndataset['LargeFamily'] = dataset['Family_size'].map(lambda s: 1 if 4 <= s else 0)\ndataset.drop(\"Family_size\", axis=1, inplace=True)\n\n# DECK\ndataset[\"Deck\"] = dataset[\"Cabin\"].str.slice(0,1)\n# Fill null values with \"N\"\ndataset[\"Deck\"] = dataset[\"Deck\"].fillna(\"N\")\n# I delete the CABIN feature because I don't need it anymore\ndataset.drop(\"Cabin\", axis=1, inplace=True)\nprint(dataset)\n\n# TITLE\nlista = []\n\ndef extraer_tratamiento(name):\n  if \".\" not in name:\n    return \"\"\n  izda, dcha = name.split(\".\", 1)\n  lista.append(izda.split()[-1])\n  return izda.split()[-1]\n\ndataset.Name.apply(extraer_tratamiento)\nlistadf = pd.DataFrame(lista)\nlistadf.rename(columns={0: \"Titulo\"}, inplace=True)\n\n# We join the dataframes\ndataset = pd.concat([dataset, listadf], axis=1)\n\n# We delete the names because they are no longer needed\ndataset.drop(\"Name\", axis=1, inplace=True)\ndataset\nprint(dataset.groupby(['Titulo'])['Titulo'].agg([\"count\"]))","metadata":{"execution":{"iopub.status.busy":"2022-07-12T17:36:41.836754Z","iopub.execute_input":"2022-07-12T17:36:41.837108Z","iopub.status.idle":"2022-07-12T17:36:41.874250Z","shell.execute_reply.started":"2022-07-12T17:36:41.837075Z","shell.execute_reply":"2022-07-12T17:36:41.872975Z"},"trusted":true},"execution_count":181,"outputs":[]},{"cell_type":"code","source":"## Transformation of \"categorical\" variables to numeric (SEX, EMBARKED, DECK, TITLE) ##\n\nsex = {\"male\": 1,\"female\": 0}\nembarked = {\"S\":0, \"C\":1, \"Q\":2}\ndeck = {\"A\":0,\"B\":1,\"C\":2,\"D\":3,\"E\":4,\"F\":5,\"G\":6,\"N\":7,\"T\":8}\ntitulo = {\"Capt\":0, \"Col\":1, \"Countess\":2,\"Don\":3, \"Dr\":4, \"Jonkheer\":5,\"Lady\":6, \"Major\":7, \"Master\":8,\n         \"Miss\":9, \"Mlle\":10, \"Mme\":11,\"Mr\":12, \"Mrs\":13, \"Ms\":14,\"Rev\":15, \"Sir\":16}\n\ndataset['Sex'] = dataset['Sex'].map(sex).astype('Int64')\ndataset['Embarked'] = dataset['Embarked'].map(embarked).astype('Int64')\ndataset['Deck'] = dataset['Deck'].map(deck).astype('Int64')\ndataset['Titulo'] = dataset['Titulo'].map(titulo).astype('Int64')\n\n# Fill the null value with \"Mr\" (which is the most common)\ndataset[\"Titulo\"] = dataset[\"Titulo\"].fillna(12)\n\ndataset = pd.get_dummies(data=dataset)\nprint(dataset)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T17:36:41.876013Z","iopub.execute_input":"2022-07-12T17:36:41.876499Z","iopub.status.idle":"2022-07-12T17:36:41.906703Z","shell.execute_reply.started":"2022-07-12T17:36:41.876464Z","shell.execute_reply":"2022-07-12T17:36:41.905576Z"},"trusted":true},"execution_count":182,"outputs":[]},{"cell_type":"code","source":"# We separate the dataset again\n\ntrain = dataset.iloc[:len(y), :]\ny_train = y\n\ntest = dataset.iloc[len(train):, :]\n\n# Now remove the \"PASSENGER ID\" feature from the \"train\" dataset, it doesn't add anything.\n\ntrain.drop(\"PassengerId\", axis=1, inplace=True)\ntrain.drop(\"Survived\", axis=1, inplace=True)\ntest.drop(\"PassengerId\", axis=1, inplace=True)\ntest.drop(\"Survived\", axis=1, inplace=True)\n\nprint(train)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T17:36:41.909469Z","iopub.execute_input":"2022-07-12T17:36:41.909804Z","iopub.status.idle":"2022-07-12T17:36:41.929302Z","shell.execute_reply.started":"2022-07-12T17:36:41.909774Z","shell.execute_reply":"2022-07-12T17:36:41.928490Z"},"trusted":true},"execution_count":183,"outputs":[]},{"cell_type":"code","source":"## Modeling ##\n\n#------------------------------------------\n# Algorithm: Decision Tree\nclf = tree.DecisionTreeClassifier(max_depth=10, random_state=42)\n# I proceed to train him\nclf.fit(train, y_train)\n# I get the predictions\ny_pred1 = clf.predict(test)\n\n#------------------------------------------\n# Algorithm: Support Vector Machine\nsvm = svm.SVC(kernel=\"poly\")\n# I proceed to train him\nsvm.fit(train, y_train)\n# I get the predictions\ny_pred2 = svm.predict(test)\n\n#------------------------------------------\n# Algorithm: Linear Discriminant Analysis\nlda = LDA(n_components=1, solver=\"eigen\",shrinkage=\"auto\", store_covariance=\"False\")\n# Adjust the scales\nX_train = lda.fit_transform(train, y_train)\nX_test = lda.transform(test)\nclassifier = LDA()\n# I proceed to train him\nclassifier.fit(train, y_train)\n# I get the predictions\ny_pred3 = classifier.predict(test)\n\n#------------------------------------------\n# Algorithm: Random forest \nrfc = RandomForestClassifier()\nparam_grid ={\n             'max_depth' : [4, 6, 11],\n                 'n_estimators': [300, 500],\n                 'max_features': ['sqrt', 'auto', 'log2'],\n                 'min_samples_split': [2, 3, 10],\n                 'min_samples_leaf': [1, 3, 10],\n                'max_leaf_nodes':st.randint(6, 10),\n                 'bootstrap': [True, False]}\n\ngrid = RandomizedSearchCV(rfc,\n                    param_grid, cv=10,\n                    scoring='accuracy',\n                    verbose=1,n_iter=10)\n\ngrid.fit(train, y_train)\ngrid.best_estimator_\ngrid.best_score_\ny_pred5 = grid.best_estimator_.predict(test)\n\n\n# The most important features of the algorithm\nfeatures = pd.DataFrame()\nfeatures['feature'] = train.columns\nfeatures['importance'] = clf.feature_importances_\nfeatures.sort_values(by=['importance'], ascending=True, inplace=True)\nfeatures.set_index('feature', inplace=True)\n\nfeatures.plot(kind='barh', figsize=(10, 7))","metadata":{"execution":{"iopub.status.busy":"2022-07-12T17:36:41.930561Z","iopub.execute_input":"2022-07-12T17:36:41.931093Z","iopub.status.idle":"2022-07-12T17:37:35.205828Z","shell.execute_reply.started":"2022-07-12T17:36:41.931062Z","shell.execute_reply":"2022-07-12T17:37:35.204662Z"},"trusted":true},"execution_count":184,"outputs":[]},{"cell_type":"code","source":"\n# We load the Y of the validation set\nY_test = pd.read_csv(\"/kaggle/input/test-file/tested.csv\")\nY_test = Y_test.Survived\n\n# Confusion matrix \"Decision Tree\"\n\nprint('\\n\\nConfusion Matrix for \"Decision Tree\":')\nmatriz_arbol = confusion_matrix(y_pred1,Y_test)\na = sns.heatmap(matriz_arbol, annot=True)\nplt.show()\nprint(\"acuracy:\", accuracy_score(y_pred1,Y_test))\nprint(\"precision:\", precision_score(y_pred1, Y_test,average='weighted'))\nprint(\"recall\" , metrics.recall_score(y_pred1, Y_test,average='weighted'))\n\n# Confusion matrix \"SVM\"\n\nprint('\\n\\nConfusion Matrix for \"SVM\":')\nmatriz_arbol = confusion_matrix(Y_test, y_pred2)\na = sns.heatmap(matriz_arbol, annot=True)\nplt.show()\nprint(\"acuracy:\", accuracy_score(Y_test, y_pred2))\nprint(\"precision:\", precision_score(Y_test, y_pred2,average='weighted'))\nprint(\"recall\" , metrics.recall_score(Y_test,y_pred2,average='weighted'))\n\n# Confusion matrix \"LDA\"\n\nprint('\\n\\nConfusion Matrix for \"LDA\":')\nmatriz_arbol = confusion_matrix(Y_test, y_pred3)\na = sns.heatmap(matriz_arbol, annot=True)\nplt.show()\nprint(\"acuracy:\", accuracy_score(Y_test, y_pred3))\nprint(\"precision:\", precision_score(Y_test, y_pred3,average='weighted'))\nprint(\"recall\" , metrics.recall_score(Y_test,y_pred3,average='weighted'))\n\n# Confusion matrix \"Random Forest\"\n\nprint('\\n\\nConfusion Matrix for \"Random Forest\":')\nmatriz_arbol = confusion_matrix(Y_test, y_pred5)\na = sns.heatmap(matriz_arbol, annot=True)\nplt.show()\nprint(\"acuracy:\", accuracy_score(Y_test, y_pred5))\nprint(\"precision:\", precision_score(Y_test, y_pred5,average='weighted'))\nprint(\"recall\" , metrics.recall_score(Y_test,y_pred5,average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-07-12T17:37:35.207690Z","iopub.execute_input":"2022-07-12T17:37:35.208054Z","iopub.status.idle":"2022-07-12T17:37:36.074088Z","shell.execute_reply.started":"2022-07-12T17:37:35.208025Z","shell.execute_reply":"2022-07-12T17:37:36.073145Z"},"trusted":true},"execution_count":185,"outputs":[]},{"cell_type":"code","source":"# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits=10)\n\n#Learning curves are a good way to see the effect of overfitting, and the effect of training size on accuracy.\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(grid.best_estimator_,\"RF mearning curves\",train, y_train,cv=kfold)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T17:37:36.075384Z","iopub.execute_input":"2022-07-12T17:37:36.076020Z","iopub.status.idle":"2022-07-12T17:37:51.803460Z","shell.execute_reply.started":"2022-07-12T17:37:36.075988Z","shell.execute_reply":"2022-07-12T17:37:51.802063Z"},"trusted":true},"execution_count":186,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame({'PassengerId': test_f.PassengerId, 'Survived': y_pred5})\n\noutput.to_csv('titanic_kaggle.csv', index=False)\n\nprint(\"Your submission was successfully saved!\")","metadata":{"execution":{"iopub.status.busy":"2022-07-12T17:37:51.805216Z","iopub.execute_input":"2022-07-12T17:37:51.805553Z","iopub.status.idle":"2022-07-12T17:37:51.815556Z","shell.execute_reply.started":"2022-07-12T17:37:51.805522Z","shell.execute_reply":"2022-07-12T17:37:51.814778Z"},"trusted":true},"execution_count":187,"outputs":[]}]}